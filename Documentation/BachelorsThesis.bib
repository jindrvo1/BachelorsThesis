
@online{EuropeanSoccerDatabase,
  title = {European {{Soccer Database}}},
  url = {https://www.kaggle.com/hugomathien/soccer},
  abstract = {25k+ matches, players \& teams attributes for European Professional Football},
  urldate = {2018-02-04},
  date = {2016},
  author = {Mathien, Hugo},
  file = {/Users/jindravo/Zotero/storage/BLTTKFM4/soccer.html}
}

@misc{PagePageRankcitationranking1998,
  title = {The {{PageRank}} Citation Ranking: {{Bringing}} Order to the {{Web}}},
  url = {http://ilpubs.stanford.edu:8090/422/1/1999-66.pdf},
  abstract = {The importance of a Web page is an inherently subjective matter, which depends on the readers interests, knowledge and attitudes. But there is still much that can be said objectively about the relative importance of Web pages. This paper describes PageRank, a mathod for rating Web pages objectively and mechanically, effectively measuring the human interest and attention devoted to them. We compare PageRank to an idealized random Web surfer. We show how to efficiently compute PageRank for large numbers of pages. And, we show how to apply PageRank to search and to user navigation.},
  date = {1998-01-29},
  author = {Page, Lawrence and Brin, Sergey and Motwani, Rajeev and Winograd, Terry},
  file = {/Users/jindravo/Zotero/storage/JBCA9HYE/1999-66.pdf}
}

@article{LazovaPageRankApproachRanking2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1503.01331},
  primaryClass = {cs},
  title = {{{PageRank Approach}} to {{Ranking National Football Teams}}},
  url = {http://arxiv.org/abs/1503.01331},
  abstract = {The Football World Cup as world's favorite sporting event is a source of both entertainment and overwhelming amount of data about the games played. In this paper we analyse the available data on football world championships since 1930 until today. Our goal is to rank the national teams based on all matches during the championships. For this purpose, we apply the PageRank with restarts algorithm to a graph built from the games played during the tournaments. Several statistics such as matches won and goals scored are combined in different metrics that assign weights to the links in the graph. Finally, our results indicate that the Random walk approach with the use of right metrics can indeed produce relevant rankings comparable to the FIFA official all-time ranking board.},
  urldate = {2018-04-08},
  date = {2015-03-04},
  keywords = {Computer Science - Social and Information Networks},
  author = {Lazova, Verica and Basnarkov, Lasko},
  file = {/Users/jindravo/Zotero/storage/LBG7PUP6/Lazova a Basnarkov - 2015 - PageRank Approach to Ranking National Football Tea.pdf;/Users/jindravo/Zotero/storage/3IJWEPUC/1503.html}
}

@book{BialkowskiWinHomeDraw,
  title = {Win at {{Home}} and {{Draw Away}}: {{Automatic Formation Analysis Highlighting}} the {{Differences}} in {{Home}} and {{Away Team Behaviors}}},
  shorttitle = {“{{Win}} at {{Home}} and {{Draw Away}}”},
  abstract = {In terms of analyzing soccer matches, two of the most important factors to consider are: 1) the formation the team played (e.g., 4-4-2, 4-2-3-1 etc.), and 2) the manner in which they executed it (e.g., conservative- sitting deep, or aggressive- pressing high). Despite the existence of ball and player tracking data, no current methods exist which can automatically detect and visualize formations. In this paper, we present an automatic method which can detect formations using an Expectation-Maximization (EM) approach. Using an entire season of Prozone data which consists of ball and player tracking information from a recent top-tier professional league, we showcase our method by investigating the “home advantage”. In a paper we published recently, using an entire season of ball tracking data we showed that home teams had significantly more possession in the forward third which correlated with more shots and goals while the shooting and passing proficiencies were the same. Using our automatic formation analysis, we extend this analysis, and show that teams tend to play the same formation at home as they do away, but the manner in which they execute it is significantly different. Specifically, we show that the formation of teams at home is significantly higher up the field compared to when they play away. This conservative approach at away games suggests that coaches aim to win their home games and draw their away games. Additionally, we also show that our method can visually summarize a game which gives an indication of dominance and tactics. While enabling new discoveries of team behavior which can enhance analysis, it is also worth mentioning that our automatic formation detection method is the first to be developed. 1},
  date = {2014},
  author = {Bialkowski, Alina and Lucey, Patrick and Carr, Peter and Yue, Yisong and Matthews, Iain},
  file = {/Users/jindravo/Zotero/storage/97D5E8YI/Bialkowski et al. - “Win at Home and Draw Away” Automatic Formation A.pdf;/Users/jindravo/Zotero/storage/I5GNR5L9/summary.html}
}

@book{PapoulisProbabilityRandomVariables1984,
  location = {{New York}},
  title = {Probability, {{Random Variables}}, and {{Stochastic Processes}}},
  edition = {2nd},
  publisher = {{McGraw-Hill}},
  date = {1984},
  author = {Papoulis, Athanasios},
  file = {/Users/jindravo/Zotero/storage/6MMLH3G3/Papoulis - 1984 - Probability, Random Variables, and Stochastic Proc.pdf}
}

@article{GOVANRANKINGNATIONALFOOTBALL,
  title = {Ranking {{National Football League Teams Using Google}}'s {{PageRank}}},
  volume = {2016},
  abstract = {The search engine Google uses the PageRanks of webpages to determine the order in which they are displayed as the result of a web search. In this work we expand Google’s idea of webpage ranking to ranking National Football League teams. We think of the teams as webpages and use the statistics of a football season to create the connections (links) between the teams. The objective is to be able to rank NFL teams by their relative strengths.},
  date = {2016},
  author = {Govan, Anjela Y and Meyer, Carl D},
  file = {/Users/jindravo/Zotero/storage/SNSTAYIJ/GOVAN a MEYER - RANKING NATIONAL FOOTBALL LEAGUE TEAMS USING GOOGL.pdf}
}

@article{deBoerTutorialCrossEntropyMethod2005,
  langid = {english},
  title = {A {{Tutorial}} on the {{Cross}}-{{Entropy Method}}},
  volume = {134},
  issn = {0254-5330, 1572-9338},
  url = {http://link.springer.com/10.1007/s10479-005-5724-z},
  doi = {10.1007/s10479-005-5724-z},
  abstract = {The cross-entropy (CE) method is a new generic approach to combinatorial and multi-extremal optimization and rare event simulation. The purpose of this tutorial is to give a gentle introduction to the CE method. We present the CE methodology, the basic algorithm and its modiﬁcations, and discuss applications in combinatorial optimization and machine learning.},
  number = {1},
  journaltitle = {Annals of Operations Research},
  urldate = {2018-04-09},
  date = {2005-02},
  pages = {19-67},
  author = {de Boer, Pieter-Tjerk and Kroese, Dirk P. and Mannor, Shie and Rubinstein, Reuven Y.},
  options = {useprefix=true},
  file = {/Users/jindravo/Zotero/storage/MLGDV242/de Boer et al. - 2005 - A Tutorial on the Cross-Entropy Method.pdf}
}

@book{Eloratingchessplayerspresent1978,
  langid = {english},
  title = {The Rating of Chessplayers, Past and Present},
  isbn = {978-0-668-04721-0},
  pagetotal = {216},
  publisher = {{Arco Pub.}},
  date = {1978},
  keywords = {Games / Chess,Chess players,Mathematics / Probability & Statistics / General,Probabilities},
  author = {Elo, Arpad E.},
  eprinttype = {googlebooks}
}

@online{RainieUseOnlineRating2004,
  langid = {american},
  title = {Use of {{Online Rating Systems}}},
  url = {http://www.pewinternet.org/2004/10/20/use-of-online-rating-systems/},
  abstract = {33 million American internet users have reviewed or rated someone or something as part of an online rating system.},
  journaltitle = {Pew Research Center: Internet, Science \& Tech},
  urldate = {2018-04-13},
  date = {2004-10-20T00:00:00+00:00},
  author = {Rainie, Lee and Hitlin, Paul},
  file = {/Users/jindravo/Zotero/storage/XI2LE5MJ/use-of-online-rating-systems.html}
}

@inproceedings{ZhangMiningmillionsreviews2012,
  langid = {english},
  title = {Mining Millions of Reviews: A Technique to Rank Products Based on Importance of Reviews},
  isbn = {978-1-4503-1428-2},
  url = {http://dl.acm.org/citation.cfm?doid=2378104.2378116},
  doi = {10.1145/2378104.2378116},
  shorttitle = {Mining Millions of Reviews},
  abstract = {As online shopping becomes increasingly more popular, many shopping web sites encourage existing customers to add reviews of products purchased. These reviews make an impact on the purchasing decisions of potential customers. At Amazon.com for instance, some products receive hundreds of reviews. It is overwhelming and time restrictive for most customers to read, comprehend and make decisions based on all of these reviews. Customers most likely end up reading only a small fraction of the reviews usually in the order which they are presented on the product page. Incorporating various product review factors, such as: content related to product quality, time of the review, content related to product durability and historically older positive customer reviews will have different impacts on the products rankings. Thus, the automated mining of product reviews and opinions to produce a re-calculated product ranking score is a valuable tool which would allow potential customers to make more informed decisions. In this paper, we present a product ranking model that applies weights to product review factors to calculate a products ranking score. Our experiments use the customer reviews from Amazon.com as input to our product ranking model which produces product ranking results that closely relate to the products sales ranking as reported by the retailer.},
  publisher = {{ACM Press}},
  urldate = {2018-04-13},
  date = {2012},
  pages = {1-8},
  author = {Zhang, Kunpeng and Cheng, Yu and Liao, Wei-keng and Choudhary, Alok},
  file = {/Users/jindravo/Zotero/storage/FT6IY5E6/Zhang et al. - 2012 - Mining millions of reviews a technique to rank pr.pdf}
}

@article{FernandezRecommendationSystemNetflix2018,
  langid = {english},
  title = {Recommendation {{System}} for {{Netflix}}},
  date = {2018},
  pages = {34},
  author = {Fernández, Leidy Esperanza Molina},
  file = {/Users/jindravo/Zotero/storage/HXNH5Y8H/Fernández - Recommendation System for Netflix.pdf}
}

@online{MoserComputingYourSkill2010,
  title = {Computing {{Your Skill}}},
  url = {http://www.moserware.com/2010/03/computing-your-skill.html},
  urldate = {2018-04-13},
  date = {2010-03-18},
  author = {Moser, Jeff},
  file = {/Users/jindravo/Zotero/storage/656ATUL8/computing-your-skill.html}
}

@article{BradleyRankAnalysisIncomplete1952,
  langid = {english},
  title = {Rank {{Analysis}} of {{Incomplete Block Designs}}: {{The Method}} of {{Paired Comparisons}}},
  volume = {39},
  issn = {0006-3444},
  url = {https://academic.oup.com/biomet/article/39/3-4/324/326091},
  doi = {10.1093/biomet/39.3-4.324},
  number = {3-4},
  journaltitle = {Biometrika},
  shortjournal = {Biometrika},
  urldate = {2018-04-13},
  date = {1952-12-01},
  pages = {324-345},
  author = {Bradley, Ralph Allan and Terry, Milton E.},
  file = {/Users/jindravo/Zotero/storage/XNZXMJV8/326091.html}
}

@article{KirkpatrickOptimizationSimulatedAnnealing1983,
  langid = {english},
  title = {Optimization by {{Simulated Annealing}}},
  volume = {220},
  issn = {0036-8075, 1095-9203},
  url = {http://science.sciencemag.org/content/220/4598/671},
  doi = {10.1126/science.220.4598.671},
  abstract = {There is a deep and useful connection between statistical mechanics (the behavior of systems with many degrees of freedom in thermal equilibrium at a finite temperature) and multivariate or combinatorial optimization (finding the minimum of a given function depending on many parameters). A detailed analogy with annealing in solids provides a framework for optimization of the properties of very large and complex systems. This connection to statistical mechanics exposes new information and provides an unfamiliar perspective on traditional optimization problems and methods.},
  number = {4598},
  journaltitle = {Science},
  urldate = {2018-04-15},
  date = {1983-05-13},
  pages = {671-680},
  author = {Kirkpatrick, S. and Gelatt, C. D. and Vecchi, M. P.},
  file = {/Users/jindravo/Zotero/storage/D2YYBWAR/671.html},
  eprinttype = {pmid},
  eprint = {17813860}
}

@article{RaoTiesPairedComparisonExperiments1967,
  eprinttype = {jstor},
  eprint = {2282923},
  title = {Ties in {{Paired}}-{{Comparison Experiments}}: {{A Generalization}} of the {{Bradley}}-{{Terry Model}}},
  volume = {62},
  issn = {0162-1459},
  doi = {10.2307/2282923},
  shorttitle = {Ties in {{Paired}}-{{Comparison Experiments}}},
  abstract = {The Bradley-Terry model for a paired-comparison experiment with t treatments postulates a set of t `true' treatment ratings π\textsubscript{1}, π\textsubscript{2}, ⋯, π\textsubscript{t} such that π\textsubscript{i} ≥ 0, Σπ\textsubscript{i} = 1 and the probability for preferring treatment i to treatment j is π\textsubscript{i}(π\textsubscript{i} + π\textsubscript{j})\textsuperscript{-1}. Thus, according to this model, every comparison of two treatments results in a definite preference for one of the two. This is an unrealistic restriction since when there is no difference between the responses due to two treatments, any method of expressing preference for one over the other is somewhat arbitrary. This paper considers a modification of the Bradley-Terry model by introducing an additional parameter, called threshold parameter, into the model. This permits `ties' in the model. The problem of estimation and tests of hypotheses for the parameters of the modified model is also dealt with in the paper.},
  number = {317},
  journaltitle = {Journal of the American Statistical Association},
  date = {1967},
  pages = {194-204},
  author = {Rao, P. V. and Kupper, L. L.}
}

@article{BujaLossFunctionsBinary2005,
  langid = {english},
  title = {Loss {{Functions}} for {{Binary Class Probability Estimation}} and {{Classiﬁcation}}: {{Structure}} and {{Applications}}},
  abstract = {What are the natural loss functions or ﬁtting criteria for binary class probability estimation? This question has a simple answer: so-called “proper scoring rules”, that is, functions that score probability estimates in view of data in a Fisher-consistent manner. Proper scoring rules comprise most loss functions currently in use: log-loss, squared error loss, boosting loss, and as limiting cases cost-weighted misclassiﬁcation losses. Proper scoring rules have a rich structure: • Every proper scoring rules is a mixture (limit of sums) of cost-weighted misclassiﬁcation losses. The mixture is speciﬁed by a weight function (or measure) that describes which misclassiﬁcation cost weights are most emphasized by the proper scoring rule.},
  date = {2005-11-03},
  pages = {49},
  author = {Buja, Andreas and Stuetzle, Werner and Shen, Yi},
  file = {/Users/jindravo/Zotero/storage/SZXSQTH7/Buja et al. - Loss Functions for Binary Class Probability Estima.pdf}
}

@book{RussellArtificialIntelligenceModern2003,
  title = {Artificial {{Intelligence}}: {{A Modern Approach}}},
  edition = {2},
  isbn = {978-0-13-790395-5},
  shorttitle = {Artificial {{Intelligence}}},
  abstract = {From the Publisher:Intelligent Agents - Stuart Russell and Peter Norvig show how intelligent agents can be built using AI methods, and explain how different agent designs are appropriate depending on the nature of the task and environment. Artificial Intelligence: A Modern Approach is the first AI text to present a unified, coherent picture of the field. The authors focus on the topics and techniques that are most promising for building and analyzing current and future intelligent systems. The material is comprehensive and authoritative, yet cohesive and readable. State of the Art - This book covers the most effective modern techniques for solving real problems, including simulated annealing, memory-bounded search, global ontologies, dynamic belief networks, neural networks, adaptive probabilistic networks, inductive logic programming, computational learning theory, and reinforcement learning. Leading edge AI techniques are integrated into intelligent agent designs, using examples and exercises to lead students from simple, reactive agents to advanced planning agents with natural language capabilities.},
  publisher = {{Pearson Education}},
  date = {2003},
  author = {Russell, Stuart J. and Norvig, Peter}
}

@book{CormenIntroductionAlgorithmsThird2009,
  title = {Introduction to {{Algorithms}}, {{Third Edition}}},
  edition = {3rd},
  isbn = {978-0-262-03384-8},
  abstract = {If you had to buy just one text on algorithms, Introduction to Algorithms is a magnificent choice. The book begins by considering the mathematical foundations of the analysis of algorithms and maintains this mathematical rigor throughout the work. The tools developed in these opening sections are then applied to sorting, data structures, graphs, and a variety of selected algorithms including computational geometry, string algorithms, parallel models of computation, fast Fourier transforms (FFTs), and more. This book's strength lies in its encyclopedic range, clear exposition, and powerful analysis. Pseudo-code explanation of the algorithms coupled with proof of their accuracy makes this book is a great resource on the basic tools used to analyze the performance of algorithms.},
  publisher = {{The MIT Press}},
  date = {2009},
  author = {Cormen, Thomas H. and Leiserson, Charles E. and Rivest, Ronald L. and Stein, Clifford}
}

@article{NovikoffConvergenceProofsPerceptrons1962,
  title = {On {{Convergence Proofs}} on {{Perceptrons}}},
  volume = {1962},
  number = {12},
  journaltitle = {Proceedings of the Symposium on the Mathematical Theory of Automata},
  date = {1962},
  pages = {615--622},
  author = {Novikoff, Albert B. J.}
}

@incollection{RumelhartNeurocomputingFoundationsResearch1988,
  location = {{Cambridge, MA, USA}},
  title = {Neurocomputing: {{Foundations}} of {{Research}}},
  isbn = {978-0-262-01097-9},
  url = {http://dl.acm.org/citation.cfm?id=65669.104451},
  shorttitle = {Neurocomputing},
  publisher = {{MIT Press}},
  urldate = {2018-04-24},
  date = {1988},
  pages = {696--699},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  editor = {Anderson, James A. and Rosenfeld, Edward}
}

@article{HornikApproximationCapabilitiesMultilayer1991,
  title = {Approximation {{Capabilities}} of {{Multilayer Feedforward Networks}}},
  volume = {4},
  issn = {0893-6080},
  url = {http://dx.doi.org/10.1016/0893-6080(91)90009-T},
  doi = {10.1016/0893-6080(91)90009-T},
  number = {2},
  journaltitle = {Neural Netw.},
  urldate = {2018-04-24},
  date = {1991-03},
  pages = {251--257},
  author = {Hornik, Kurt}
}

@article{SrivastavaDropoutSimpleWay2014,
  title = {Dropout: {{A Simple Way}} to {{Prevent Neural Networks}} from {{Overfitting}}},
  volume = {15},
  url = {http://jmlr.org/papers/v15/srivastava14a.html},
  shorttitle = {Dropout},
  journaltitle = {Journal of Machine Learning Research},
  urldate = {2018-04-24},
  date = {2014},
  pages = {1929-1958},
  author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  file = {/Users/jindravo/Zotero/storage/WUA8KFT7/Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks f.pdf;/Users/jindravo/Zotero/storage/4TYLRYE4/srivastava14a.html}
}

@online{WorldChessFederationRatingRegulationsKFactor2009,
  title = {Rating {{Regulations}} - {{The K}}-{{Factor}}},
  url = {http://www.fide.com/component/content/article/1-fide-news/3963-rating-regulations-the-k-factor},
  urldate = {2018-04-25},
  date = {2009-04-24},
  author = {World Chess Federation},
  file = {/Users/jindravo/Zotero/storage/D3IKFCQX/3963-rating-regulations-the-k-factor.html}
}

@online{UnitedStatesChessFederationUnitedStatesChess2013,
  title = {The {{United States Chess Federation}} - {{K}}-{{Factor Change}} - {{May}} 2013},
  url = {http://www.uschess.org/content/view/12202/141/},
  journaltitle = {USCHESS},
  urldate = {2018-04-25},
  date = {2013-03-13},
  author = {United States Chess Federation},
  file = {/Users/jindravo/Zotero/storage/43N7C55N/141.html}
}

@article{RubinsteinCrossEntropyMethodCombinatorial1999,
  langid = {english},
  title = {The {{Cross}}-{{Entropy Method}} for {{Combinatorial}} and {{Continuous Optimization}}},
  volume = {1},
  issn = {1387-5841, 1573-7713},
  url = {https://link.springer.com/article/10.1023/A:1010091220143},
  doi = {10.1023/A:1010091220143},
  abstract = {We present a new and fast method, called the cross-entropy method, for finding the optimal solution of combinatorial and continuous nonconvex optimization problems with convex bounded domains. To find the optimal solution we solve a sequence of simple auxiliary smooth optimization problems based on Kullback-Leibler cross-entropy, importance sampling, Markov chain and Boltzmann distribution. We use importance sampling as an important ingredient for adaptive adjustment of the temperature in the Boltzmann distribution and use Kullback-Leibler cross-entropy to find the optimal solution. In fact, we use the mode of a unimodal importance sampling distribution, like the mode of beta distribution, as an estimate of the optimal solution for continuous optimization and Markov chains approach for combinatorial optimization. In the later case we show almost surely convergence of our algorithm to the optimal solution. Supporting numerical results for both continuous and combinatorial optimization problems are given as well. Our empirical studies suggest that the cross-entropy method has polynomial in the size of the problem running time complexity.},
  number = {2},
  journaltitle = {Methodology And Computing In Applied Probability},
  shortjournal = {Methodology and Computing in Applied Probability},
  urldate = {2018-04-26},
  date = {1999-09-01},
  pages = {127-190},
  author = {Rubinstein, Reuven},
  file = {/Users/jindravo/Zotero/storage/PRNE3YF6/10.html}
}

@article{RosenblattPerceptronProbabilisticModel1958,
  title = {The {{Perceptron}}: {{A Probabilistic Model}} for {{Information Storage}} and {{Organization}} in {{The Brain}}},
  shorttitle = {The {{Perceptron}}},
  abstract = {If we are eventually to understand the capability of higher organisms for perceptual recognition, generalization, recall, and thinking, we must first have answers to three fundamental questions: 1. How is information about the physical world sensed, or detected, by the biological system? 2. In what form is information stored, or remembered? 3. How does information contained in storage, or in memory, influence recognition and behavior? The first of these questions is in the},
  journaltitle = {Psychological Review},
  date = {1958},
  pages = {65--386},
  author = {Rosenblatt, F.},
  file = {/Users/jindravo/Zotero/storage/F4DLNL69/Rosenblatt - 1958 - The Perceptron A Probabilistic Model for Informat.pdf;/Users/jindravo/Zotero/storage/PTKGNEDQ/summary.html}
}

@online{USASwimming2018USASwimming2018,
  title = {2018 {{USA Swimming Rules}} and {{Regulations}}},
  journaltitle = {www.usaswimming.org},
  date = {2018},
  author = {USA Swimming},
  file = {/Users/jindravo/Zotero/storage/3S2GFBDJ/2018-rulebook.pdf}
}

@inproceedings{ReganUnderstandingDistributionsChess2011,
  langid = {english},
  title = {Understanding {{Distributions}} of {{Chess Performances}}},
  isbn = {978-3-642-31865-8 978-3-642-31866-5},
  url = {https://link.springer.com/chapter/10.1007/978-3-642-31866-5_20},
  doi = {10.1007/978-3-642-31866-5_20},
  abstract = {This paper studies the population of chess players and the distribution of their performances measured by Elo ratings and by computer analysis of moves. Evidence that ratings have remained stable since the inception of the Elo system in the 1970’s is given in three forms: (1) by showing that the population of strong players fits a straightforward logistic-curve model without inflation, (2) by plotting players’ average error against the FIDE category of tournaments over time, and (3) by skill parameters from a model that employs computer analysis keeping a nearly constant relation to Elo rating across that time. The distribution of the model’s Intrinsic Performance Ratings can therefore be used to compare populations that have limited interaction, such as between players in a national chess federation and FIDE, and ascertain relative drift in their respective rating systems.},
  eventtitle = {Advances in {{Computer Games}}},
  booktitle = {Advances in {{Computer Games}}},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer, Berlin, Heidelberg}},
  urldate = {2018-05-05},
  date = {2011-11-20},
  pages = {230-243},
  author = {Regan, Kenneth W. and Macieja, Bartlomiej and Haworth, Guy McC},
  file = {/Users/jindravo/Zotero/storage/Y6GCQHG4/978-3-642-31866-5_20.html}
}

@inproceedings{CaruanaOverfittingNeuralNets2000,
  title = {Overfitting in {{Neural Nets}}: {{Backpropagation}}, {{Conjugate Gradient}}, and {{Early Stopping}}},
  shorttitle = {Overfitting in {{Neural Nets}}},
  abstract = {The conventional wisdom is that backprop nets with excess hidden units  generalize poorly. We show that nets with excess capacity generalize  well when trained with backprop and early stopping. Experiments suggest  two reasons for this: 1) Overfitting can vary significantly in different  regions of the model. Excess capacity allows better fit to regions of high  non-linearity, and backprop often avoids overfitting the regions of low  non-linearity. 2) Regardless of size, nets learn task subcomponents in  similar sequence. Big nets pass through stages similar to those learned  by smaller nets. Early stopping can stop training the large net when it  generalizes comparably to a smaller net. We also show that conjugate  gradient can yield worse generalization because it overfits regions of low  non-linearity when learning to fit regions of high non-linearity.},
  booktitle = {In {{Proc}}. {{Neural Information Processing Systems Conference}}},
  date = {2000},
  pages = {402--408},
  author = {Caruana, Rich and Lawrence, Steve and Giles, Lee},
  file = {/Users/jindravo/Zotero/storage/ZHYJ9Y97/Caruana et al. - 2000 - Overfitting in Neural Nets Backpropagation, Conju.pdf;/Users/jindravo/Zotero/storage/WYYNQQUB/summary.html}
}


